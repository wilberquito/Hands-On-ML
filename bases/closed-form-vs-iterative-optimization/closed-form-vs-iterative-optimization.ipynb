{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Form vs Iterative Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaly there are two ways to train a model. Using a \"closed form\" equation, i.e., a equation composed by a finite number of constants, variables and operations, there are no inifinite sums, no limits, no integrals, etc. \n",
    "\n",
    "On the other hand, there is the approach that consists on tweaking the models parameters to minimize a cost function over a training set. Eventually this twiking process convers to the same parameters as the first method. The iterative process is known as gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model makes predictions by simply computing a weighted sum of the input features plus a constant called the *bias*.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n\n",
    "$$\n",
    "\n",
    "This equation can be written much more concisely using a vectorized form.\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_{\\theta}(x) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $h_{\\theta}$ is the hypothesis function, using the model parameters $\\theta$.\n",
    "- $\\theta$ is the model's parameters vector, containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$.\n",
    "- $x$ is the instance's features vector, containing $x_0$ to $x_n$, where $x_0$ always equals to 1.\n",
    "- $\\theta \\cdot x$ is the dot product of the vector $\\theta$ and $x$.\n",
    "\n",
    "\n",
    "If $\\theta$ and $x$ are column vectors then the predictions is $\\hat{y} = \\theta^Tx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that training a model means setting its parameters so that the model best fits the training set. So, we need to first mesure how well or poorly the model fits the training data. To mesure how well a an algorithm is performing in training we use a function called \"loss function\", which generally is a different function than the one used to compute how well the algorithm is performing on predicting values, this other function is called the \"objective function\".\n",
    "\n",
    "For instance, in case of a linear regression hypothesis $h_{\\theta}$, we usually use the mean standard error *MSE* as a cost function, and the objective of a training a linear regression is minimize this function.\n",
    "\n",
    "$$MSE(X, h_{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m}{(\\theta^{T}x^{(i)} - y^{(i)})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we minimize a function? we can use derivates for that... What we really should care to minimize is the expression $\\digamma$, where $\\digamma(\\theta) = (\\theta^{T}x^{(i)} - y^{(i)})^2$. We can use the **chain rule** which is usuful when we want to derivate functions that contains other functions.\n",
    "\n",
    "$$[f(g(x))]' = f'(g(x))*g'(x) - \\text{chain rule}$$\n",
    "\n",
    "So, the derivate of function $\\digamma$ respect to the variable $\\theta$ and it looks like following:\n",
    "\n",
    "$$\\frac{\\partial{}}{\\partial{\\theta}}\\digamma{(\\theta)} = 2(\\theta^{T}x^{(i)} - y^{(i)}) * (x^{(i)})\n",
    "$$\n",
    "\n",
    "Hence, the partial derivate of the cost function to respect to the hypothesis $h_{\\theta}$:\n",
    "\n",
    "$$\\frac{\\partial{}}{\\partial{\\theta}}MSE(X, h_{\\theta}) = \\frac{\\partial{}}{\\partial{\\theta}}MSE(\\digamma{(\\theta)}) =  \\frac{2}{m}\\sum_{i=1}^{m}{(\\theta^{T}x^{(i)} - y^{(i)}) * (x^{(i)})}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought it seems very interesting what we really want to find are the values of $\\theta$ that minimizes the cost function. We could do that again by minimizing the $J(\\theta)$, where $J(\\theta) = (\\theta^{T}x^{(i)} - y^{(i)})$, because the other values are constants and can not be minimized. We can show the cost function as a vector.\n",
    "\n",
    "$$\n",
    "J(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\theta^T(x^0) \\\\\n",
    "\\theta^T(x^1) \\\\\n",
    "\\dots \\\\\n",
    "\\theta^T(x^m) \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "- y \n",
    "= X\\theta - y\n",
    "$$\n",
    "\n",
    "To compute the residuals of that matrix we need to lift it to the square.\n",
    "\n",
    "$$\n",
    "J(\\theta) = (X\\theta - y)^T(X\\theta - y),\n",
    "\\\\\n",
    "J(\\theta) = (X\\theta)^T(X\\theta) - 2y^T(X\\theta) + y^Ty\n",
    "$$\n",
    "\n",
    "Differentiate with respect of $\\theta$ of the functions $a(\\theta) = (X\\theta)^T(X\\theta), \\ b(\\theta) = - 2y^T(X\\theta) \\ \\text{and} \\ c(\\theta) = y^Ty$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial{}}{\\partial{\\theta}}a(\\theta) = 2X^T(X\\theta),\n",
    "\\hspace{0.25cm}\n",
    "\\frac{\\partial{}}{\\partial{\\theta}}b(\\theta) =  - 2X^Ty\n",
    "\\hspace{0.25cm}\n",
    "\\frac{\\partial{}}{\\partial{\\theta}}c(\\theta) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, putting all togueter:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{}}{\\partial{\\theta}}J(\\theta) = 2X^TX\\theta - 2X^Ty\n",
    "$$\n",
    "\n",
    "To find the values of the parameters that minimize the loss function, we set the gradients to zero.\n",
    "\n",
    "$$\n",
    "2X^TX\\theta - 2X^Ty = 0\n",
    "\\\\\n",
    "X^TX\\theta = X^Ty \n",
    "$$\n",
    "\n",
    "Assuming $X^TX$ is invertible, solve for $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
